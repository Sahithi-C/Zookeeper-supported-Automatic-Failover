#  Zookeeper-supported Automatic Failover in a Master-Worker Distributed Execution 

## Introduction:

This project uses Zookeeper to implement an automatic failover mechanism in a master-worker distributed execution of graph-bridge programs. Client.java, (i.e., the master) connects to ZooKeeper; submits 10 different tasks, each executing GraphBride.java with a different graph size; and waits for all tasks to be completed by remote worker processes. Each worker which should be implemented in Worker.java joins Zookeeper; repetitively starts a new task; and fails over a task if it is not done within 100 seconds. Distributed synchronization must be implemented in Key.java to have all the workers access the bag of tasks exclusively.

<img width="515" alt="image" src="https://github.com/user-attachments/assets/f41e9169-02e7-41f1-8dbd-aeef1792e854" />

## Documentation:

**Client.java** - Connects to ZooKeeper at a given TCP port in args[0], creates the /workers and the /tasks nodes persistently; submits 10 tasks under /tasks where each task has “submitted” as its data and is identified with task-000000000d where d = 1, 2, …, 10; launches 10 event watchers, each checking a task deletion from /tasks; and finally deletes /workers and /tasks from ZooKeeper upon no more tasks under /tasks.  

**Worker.java** - Joins ZooKeeper at a given TCP port in args[0]; registers itself as worker 00000000d where d = 1, 2, …, 10 under/workers; and repeats picking up a new task until /tasks becomes empty. For each task picked up, Worker.java checks its data: “submitted” allows the worker to update its data with the current timestamp and to launch a new GraphBridge program, otherwise the data should be a past timestamp when someone else picked up this task. In that case, Worker.java checks if the task is overdue beyond 100,000msec. If so, let’s restart this task. Otherwise, Worker.java simply leaves the current task execution, assuming that the task may be stalled. 

**Key.java** - Implements lock( ) and unlock( ). They are used for each worker to exclusively access the /tasks node when picking up and updating a task 000000000d in a non-interruptive fashion. The lock( ) method creates the /lock node. If the node has been already created, (i.e., someone else has locked), the worker launches an event watcher and waits on itself. The event watcher is woken up upon a deletion of /lock and notify the worker. The unlock( ) method simply deletes the /lock node. 

**GraphBridge.java** - Is a Java application to be executed by Worker.java. It receives the number of vertices, (say N) in args[0]; generates a random graph with N vertices; and find all graph bridges using depth-first search. You don’t have to understand the details of the algorithm. 

## Discussion:

**Worker.java** - The Worker class handles task pickup, execution, and completion. The pickupTask() method implements the main logic for workers to compete for available tasks using distributed locking. When a worker wants to pick up a task, it first acquires a lock using Key.lock() to ensure exclusive access to the task queue. It then examines each task's status, where "submitted" indicates an available task, while a timestamp indicates that another worker is currently processing it. The system implements automatic failover by checking if a task with a timestamp is overdue beyond 100 seconds, allowing other workers to take over stalled tasks. When claiming a task, the worker updates its status with the current timestamp to indicate ownership. The runTask() method handles the actual execution of tasks by converting the task ID into a number of vertices for the GraphBridge program. It launches GraphBridge as a separate process using Runtime.exec(), captures and displays the program output, and waits for the task to complete. The finishTask() method ensures proper cleanup by retrieving the latest version of the completed task and deleting it from ZooKeeper using the correct version number to avoid conflicts with other workers who might be trying to access the same task.

**Key.java** - The Key class provides locking mechanism that ensures mutual exclusion when workers access the task queue. The lock() method attempts to create a lock node in ZooKeeper, and if successful, the worker obtains the lock and can proceed with task operations. If the lock already exists, the method sets up a watcher and uses Java's wait/notify for thread synchronization, causing the worker to wait until the lock becomes available. The lockWatcher monitors lock deletion events and notifies waiting workers when the lock is released, ensuring that only one worker can access the task queue at any given time. The unlock() method simply deletes the lock node, allowing other waiting workers to acquire the lock and continue their operations.

**Additional Feature** - For additional feature, I created Client2.java that extends the original system's capacity from 10 tasks to 20 tasks by modifying the nTasksSubmitted variable from 10 to 20, updating task path formatting to handle the increased number of tasks, and enhancing progress tracking to show completion status. This feature tests the system's scalability and shows that the distributed locking mechanism remains reliable under higher load conditions.

## Limitations and Improvements:
1. One limitation is that the number of tasks is still hardcoded to 20. A possible improvement would be to let the user input the number of tasks dynamically through the command line. 
2. Another limitation is that all tasks are submitted at once, which could overwhelm the system if a large number of tasks is chosen. A better approach would be to submit tasks in batches or based on worker availability, improving resource management and system stability.
3. A further improvement would be to implement dynamic task monitoring using getChildren() with a watcher on /tasks. This would avoid setting up watchers on each individual task and would scale better when hundreds of tasks are created. 
4. Additionally, adding retry logic or error reporting for failed task submissions or deletions could make the program more reliable in real-world distributed environments where transient issues often occur.
